{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOdxR/UM65/NHNhYa1dq0tP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":[],"metadata":{"id":"hk6ak32SNFE6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> 모델 학습\n","\n"],"metadata":{"id":"6pCiGpqBNDjn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"1O_7pt-EEh5e"},"outputs":[],"source":["#모델 학습 및 테스트\n","import pandas as pd\n","data = pd.read_excel('mlb_dp_20.xlsx')\n","\n","\n","from xgboost import XGBRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score\n","from sklearn.model_selection import KFold, cross_val_predict\n","kfold = KFold( n_splits=5, shuffle=True, random_state = 1234)\n","\n","X = data.drop(columns=[\"theta_p\", \"distance\", \"theta_n\", \"Unnamed: 0\"])\n","y = data[[\"theta_p\", \"distance\"]]\n","\n","# 1차: Train(80%) + Test(20%) 분할\n","x_train_val, x_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 2차: Train(60%) + Validation(20%) 분할 (Train+Validation을 다시 나눔)\n","x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=42)\n","\n","# XGBoost 다중 출력 회귀 모델\n","xgb_model = MultiOutputRegressor( XGBRegressor(random_state=1234, n_jobs=-1))\n","xgb_model.fit(x_train, y_train)\n","\n","x_train = x_train.reset_index(drop=True)\n","y_train = y_train.reset_index(drop=True)\n","\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","def rmse(y_true, y_pred):\n","    return np.sqrt(mean_squared_error(y_true, y_pred))\n","\n","def mse(y_true, y_pred):\n","    return mean_squared_error(y_true, y_pred)\n","\n","def mae(y_true, y_pred):\n","    return mean_absolute_error(y_true, y_pred)\n","\n","def mape(y_true, y_pred):\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100  # 백분율로 변환\n","\n","fold = 1\n","msel = []\n","mael = []\n","rmsel = []\n","mapel = []\n","for train_index, val_index in kfold.split(x_train):\n","    X_train_fold, X_val_fold = x_train.iloc[train_index], x_train.iloc[val_index]\n","    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n","\n","    # 모델 훈련\n","    xgb_model.fit(X_train_fold, y_train_fold)\n","\n","    # 예측\n","    y_pred = xgb_model.predict(X_val_fold)\n","\n","    # 평가 지표 계산\n","    fold_mse = mse(y_val_fold, y_pred)\n","    fold_mae = mae(y_val_fold, y_pred)\n","    fold_rmse = rmse(y_val_fold, y_pred)\n","    fold_mape = mape(y_val_fold, y_pred)\n","\n","    msel.append(fold_mse)\n","    mael.append(fold_mae)\n","    rmsel.append(fold_rmse)\n","    mapel.append(fold_mape)\n","\n","    # 결과 출력\n","    print(f\"Fold {fold}:\")\n","    print(f\"  MSE: {fold_mse:.4f}\")\n","    print(f\"  MAE: {fold_mae:.4f}\")\n","    print(f\"  RMSE: {fold_rmse:.4f}\")\n","    print(f\"  MAPE: {fold_mape:.4f}%\\n\")\n","\n","    fold += 1\n","\n","import numpy as np\n","print(np.mean(msel))\n","print(np.mean(mael))\n","print(np.mean(rmsel))\n","print(np.mean(mapel))"]},{"cell_type":"code","source":[],"metadata":{"id":"WVUagy5kEpgM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> 하이퍼 파라미터 최적화\n","\n"],"metadata":{"id":"hOvLCiKKNKae"}},{"cell_type":"code","source":["# HPT (Optuna)\n","import optuna\n","import numpy as np\n","import pickle\n","from optuna.samplers import TPESampler\n","from sklearn.model_selection import KFold\n","\n","from xgboost import XGBRegressor\n","from sklearn.multioutput import MultiOutputRegressor\n","\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import KFold\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","\n","# RMSE 및 MAPE 계산 함수 정의\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","\n","def rmse(y_true, y_pred):\n","    return np.sqrt(mean_squared_error(y_true, y_pred))\n","\n","def mape(y_true, y_pred):\n","    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n","\n","# KFold 설정\n","kfold = KFold(n_splits=5, random_state=1234, shuffle=True)\n","\n","# Optuna 목적 함수 정의\n","def objective(trial):\n","    params = {\n","        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000, step=50),\n","        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 30),\n","        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.005, 0.1),\n","        \"subsample\": trial.suggest_uniform(\"subsample\", 0.6, 1.0),\n","        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.7, 0.9),\n","        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-4, 5.0),\n","        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-3, 5.0),\n","        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 7, 12),\n","        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-2, 7.0)\n","    }\n","\n","    X = data.drop(columns=[\"theta_p\", \"distance\", \"theta_n\", \"Unnamed: 0\"])\n","    y = data[[\"theta_p\", \"distance\"]]\n","\n","    # 1차: Train(80%) + Test(20%) 분할\n","    x_train_val, x_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    # 2차: Train(60%) + Validation(20%) 분할 (Train+Validation을 다시 나눔)\n","    x_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=42)\n","\n","\n","    xgb_model = MultiOutputRegressor(XGBRegressor(random_state=1234, n_jobs=-1))\n","\n","    rmse_scores, mape_scores = [], []\n","\n","    for train_idx, val_idx in kfold.split(x_train):\n","        X_t, X_v = x_train.iloc[train_idx], x_train.iloc[val_idx]\n","        y_t, y_v = y_train.iloc[train_idx], y_train.iloc[val_idx]\n","\n","        xgb_model.fit(X_t, y_t)\n","        y_pred = xgb_model.predict(X_v)\n","\n","        rmse_scores.append(rmse(y_v, y_pred))\n","        mape_scores.append(mape(y_v, y_pred))\n","\n","    # RMSE와 MAPE를 동시에 최적화하기 위해 가중 평균을 사용\n","    rmse_mean = np.mean(rmse_scores)\n","    mape_mean = np.mean(mape_scores)\n","\n","    return 0.7 * rmse_mean + 0.3 * (mape_mean / 100)  # 가중합 (MAPE를 %에서 소수로 변환)\n","\n","# Optuna 실행\n","xgb_study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=1234))\n","xgb_study.optimize(objective, n_trials=50)\n","\n","# 최적 하이퍼파라미터 출력\n","print(\" 최적 하이퍼파라미터 (Optuna):\", xgb_study.best_params)\n","print(\" 최적 RMSE + MAPE 가중합 (Optuna):\", xgb_study.best_trial.value)\n","\n","# 최적 모델 저장\n","opt_xgb =  MultiOutputRegressor(XGBRegressor(random_state=1234, **xgb_study.best_params))\n","file_name = \"xgb_model.pkl\"\n","with open(file_name, \"wb\") as f:\n","    pickle.dump(opt_xgb, f)\n","print(f\" 모델이 {file_name}에 저장되었습니다.\")"],"metadata":{"id":"PP7luNPUEpdm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> 최종 모델 성능\n","\n"],"metadata":{"id":"JoXfRGZmM-GY"}},{"cell_type":"code","source":["# 최종 모델 성능\n","from joblib import load\n","xgb_model = load(\"xgb_model.pkl\")\n","\n","X = data.drop(columns=[\"theta_p\", \"distance\", \"theta_n\", \"Unnamed: 0\"])\n","y = data[[\"theta_p\", \"distance\"]]\n","\n","# 1차: Train(80%) + Test(20%) 분할\n","x_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# 2차: Train(60%) + Validation(20%) 분할 (Train+Validation을 다시 나눔)\n","X_train, x_val, y_train, y_val = train_test_split(x_train_val, y_train_val, test_size=0.25, random_state=42)\n","\n","xgb_model.fit(X_train, y_train)\n","print(\"XGBoost RMSE : \", rmse(y_test, xgb_model.predict(X_test)))\n","print(\"XGBoost MAPE : \", mape(y_test, xgb_model.predict(X_test)))"],"metadata":{"id":"yJ75lJcnFoX8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KJsY9Cp_FoQu"},"execution_count":null,"outputs":[]}]}